{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fatbot as fb\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import fatbot.db4 as db\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbalgo =                fb.PPO         #<----- model args depend on this\n",
    "model_name =            'ppo_modelzen'    #<----- stores data in this folder\n",
    "os.makedirs(model_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_scheme = dict( \n",
    "                #dis_target_point=   1.0, \n",
    "                #dis_target_radius=  1.0, \n",
    "                dis_neighbour = 1.0,\n",
    "                all_unsafe=         1.0, \n",
    "                all_neighbour=      1.0, \n",
    "                occluded_neighbour= 2.0, \n",
    "                )\n",
    "delta_reward = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma =                 0.5\n",
    "horizon =               30\n",
    "total_timesteps =       100_000 #<--- for training\n",
    "model_version =         'zen'\n",
    "model_path =            os.path.join(model_name, model_version)\n",
    "\n",
    "# learning rate scheduling\n",
    "start_lr, end_lr = 0.00050, 0.000040\n",
    "lr_mapper=fb.REMAP((-0.2,1), (start_lr, end_lr)) # set learn rate schedluer\n",
    "def lr_schedule(progress):\n",
    "  #progress_precent = 100*(1-progress)\n",
    "  #lr = lr_mapper.in2map(1-progress)\n",
    "  #if int(progress_precent) % 10 == 0:\n",
    "  #  print(f'Progress: {progress} ~~> {progress_precent:.3f} %,  {lr = }')  \n",
    "  return lr_mapper.in2map(1-progress) #lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model='ppo_model'\n",
    "\n",
    "initial_state_keys=[]\n",
    "\n",
    "for i in range(11):\n",
    "  ifile = f'base_{i}_final.npy'\n",
    "  initial_state_keys.append(list(np.load(os.path.join(load_model, ifile))))\n",
    "len(initial_state_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# initial state distribution - uniformly sample from all listed states\n",
    "#initial_state_keys =  db.all_states() # [db.isd[db.isd_keys[0]]] #[v for k,v in db.isd.items()] \n",
    "permute_states =        False\n",
    "#print(f'Total Initial States: {len(initial_state_keys)}')\n",
    "\n",
    "# build training env\n",
    "training_env = db.envF(False, horizon, reward_scheme, delta_reward, \n",
    "                        permute_states, *initial_state_keys)\n",
    "\n",
    "#<---- optinally check\n",
    "fb.check_env(training_env) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "training_start_time = fb.common.now()\n",
    "print(f'Training @ [{model_path}]')\n",
    "model = sbalgo(policy=      'MlpPolicy', \n",
    "        env=                training_env, \n",
    "        learning_rate =     lr_schedule,\n",
    "        n_steps=            1024,\n",
    "        batch_size =        32,\n",
    "        n_epochs =          10,\n",
    "        gamma =             gamma,\n",
    "        gae_lambda=         0.95,\n",
    "        clip_range=         0.25, \n",
    "        clip_range_vf=      None, \n",
    "        normalize_advantage=True, \n",
    "        ent_coef=           0.0, \n",
    "        vf_coef=            0.5, \n",
    "        max_grad_norm=      0.5, \n",
    "        use_sde=            False, \n",
    "        sde_sample_freq=    -1, \n",
    "        target_kl=          None, \n",
    "        tensorboard_log=    None, \n",
    "        create_eval_env=    False, \n",
    "        verbose=            0, \n",
    "        seed=               None, \n",
    "        device=             'cpu', \n",
    "        _init_setup_model=  True,\n",
    "        policy_kwargs=dict(\n",
    "                        activation_fn=  nn.LeakyReLU, \n",
    "                        net_arch=[dict(\n",
    "                            pi=[400, 300, 300, 300], \n",
    "                            vf=[400, 300, 300, 300])])) #256, 256, 256, 128, 128\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps,log_interval=int(0.1*total_timesteps))\n",
    "model.save(model_path)\n",
    "training_end_time = fb.common.now()\n",
    "print(f'Finished!, Time-Elapsed:[{training_end_time-training_start_time}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sbalgo.load(model_path)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial state distribution - uniformly sample from all listed states\n",
    "#initial_state_keys =    db.all_states() # [db.isd[db.isd_keys[0]]]  #[v for k,v in db.isd.items()] \n",
    "permute_states =        False\n",
    "nos_initial_states=len(initial_state_keys)\n",
    "print(f'Total Initial States: {nos_initial_states}')\n",
    "\n",
    "# build testing_env\n",
    "testing_env = db.envF(True, horizon, reward_scheme, delta_reward, \n",
    "                        permute_states, *initial_state_keys)\n",
    "\n",
    "# save initial state\n",
    "#testing_env.reset()\n",
    "#testing_env.save_state(f'{model_path}_initial.npy')\n",
    "#fig=testing_env.render() # do a default render\n",
    "#fig.savefig(f'{model_path}_initial.png')\n",
    "#del fig\n",
    "\n",
    "#<---- optinally check\n",
    "#fb.check_env(training_env) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Testing @ [{model_path}]')\n",
    "average_return, total_steps = fb.TEST(\n",
    "    env=            testing_env, \n",
    "    model=          sbalgo.load(model_path), #<---- use None for random\n",
    "    episodes=       11,\n",
    "    steps=          0,\n",
    "    deterministic=  True,\n",
    "    render_as=      'zen', # use None for no plots, use '' (empty string) to plot inline\n",
    "    save_dpi=       'figure',\n",
    "    make_video=     False,\n",
    "    video_fps=      2,\n",
    "    render_kwargs=dict(local_sensors=True, reward_signal=True),\n",
    "    starting_state=lambda ep: ep, # either none or lambda episode: initial_state_index (int)\n",
    "    plot_results=0,\n",
    "    start_n=0, # for naming render pngs\n",
    "    save_state_info=model_path, # call plt.show() if true\n",
    "    save_both_states=False,\n",
    ")\n",
    "print(f'{average_return=}, {total_steps=}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Welcome.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
