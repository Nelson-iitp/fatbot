{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fatbot as fb\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbalgo =                fb.PPO         #<----- model args depend on this\n",
    "model_name =            'ppo_model'    #<----- stores data in this folder\n",
    "os.makedirs(model_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_scheme = 'hull'\n",
    "delta_reward = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma =                 0.99\n",
    "horizon =               300\n",
    "total_timesteps =       10_000 #<--- for training\n",
    "model_version =         'baseZ'\n",
    "model_path =            os.path.join(model_name, model_version)\n",
    "\n",
    "# learning rate scheduling\n",
    "start_lr, end_lr = 0.00050, 0.000040\n",
    "lr_mapper=fb.REMAP((-0.2,1), (start_lr, end_lr)) # set learn rate schedluer\n",
    "def lr_schedule(progress):\n",
    "  #progress_precent = 100*(1-progress)\n",
    "  #lr = lr_mapper.in2map(1-progress)\n",
    "  #if int(progress_precent) % 10 == 0:\n",
    "  #  print(f'Progress: {progress} ~~> {progress_precent:.3f} %,  {lr = }')  \n",
    "  return lr_mapper.in2map(1-progress) #lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Initial States: 6\n",
      "[*] World Created :: world_db6 :: Dim: ( X=60.0, Y=60.0, H=300 )\n",
      "Delta-Reward: [True],  Delta-Action: [False]\n",
      "Imaging: [False],  History: [False]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sparrow/qpdb/pie/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "db = fb.db.db6\n",
    "# initial state distribution - uniformly sample from all listed states\n",
    "initial_state_keys =  db.isd['S'] # [db.isd[db.isd_keys[0]]] #[v for k,v in db.isd.items()] \n",
    "permute_states =        False\n",
    "print(f'Total Initial States: {len(initial_state_keys)}')\n",
    "\n",
    "# build training env\n",
    "training_env = db.envF(False, 20, 0, horizon, reward_scheme, delta_reward, \n",
    "                         initial_state_keys, False)\n",
    "\n",
    "#<---- optinally check\n",
    "fb.check_env(training_env) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-20.,  20.,   0.,   0.,   1.,   0.,   0.,  20.,   0.,   0.,   2.,\n",
       "         0.,  20.,  20.,   0.,   0.,   1.,   0., -20., -20.,   0.,   0.,\n",
       "         1.,   0.,   0., -20.,   0.,   0.,   2.,   0.,  20., -20.,   0.,\n",
       "         0.,   1.,   0.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training @ [ppo_model/baseZ]\n",
      "Finished!, Time-Elapsed:[0:00:28.953694]\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "training_start_time = fb.common.now()\n",
    "print(f'Training @ [{model_path}]')\n",
    "model = sbalgo(policy=      'MlpPolicy', \n",
    "        env=                training_env, \n",
    "        learning_rate =     lr_schedule,\n",
    "        n_steps=            1024*10,\n",
    "        batch_size =        1024,\n",
    "        n_epochs =          20,\n",
    "        gamma =             gamma,\n",
    "        gae_lambda=         0.95,\n",
    "        clip_range=         0.20, \n",
    "        clip_range_vf=      None, \n",
    "        normalize_advantage=True, \n",
    "        ent_coef=           0.0, \n",
    "        vf_coef=            0.5, \n",
    "        max_grad_norm=      0.5, \n",
    "        use_sde=            False, \n",
    "        sde_sample_freq=    -1, \n",
    "        target_kl=          None, \n",
    "        tensorboard_log=    None, \n",
    "        create_eval_env=    False, \n",
    "        verbose=            0, \n",
    "        seed=               None, \n",
    "        device=             'cpu', \n",
    "        _init_setup_model=  True,\n",
    "        policy_kwargs=dict(\n",
    "                        activation_fn=  nn.LeakyReLU, \n",
    "                        net_arch=[dict(\n",
    "                            pi=[512, 512, 300], \n",
    "                            vf=[512, 512, 300])])) #256, 256, 256, 128, 128\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps,log_interval=int(0.1*total_timesteps))\n",
    "model.save(model_path)\n",
    "training_end_time = fb.common.now()\n",
    "print(f'Finished!, Time-Elapsed:[{training_end_time-training_start_time}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<stable_baselines3.ppo.ppo.PPO at 0x7f7110724e80>, 'ppo_model/baseZ')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sbalgo.load(model_path)\n",
    "model, model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Initial States: 6\n",
      "[*] World Created :: world_db6 :: Dim: ( X=60.0, Y=60.0, H=50 )\n",
      "Delta-Reward: [True],  Delta-Action: [False]\n",
      "Imaging: [False],  History: [True]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sparrow/qpdb/pie/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# initial state distribution - uniformly sample from all listed states\n",
    "initial_state_keys =  db.isd['S'] # [db.isd[db.isd_keys[0]]] #[v for k,v in db.isd.items()] \n",
    "permute_states =        False\n",
    "print(f'Total Initial States: {len(initial_state_keys)}')\n",
    "\n",
    "# build training env\n",
    "testing_env = db.envF(True, 20, 0, 50, reward_scheme, delta_reward, \n",
    "                         initial_state_keys, False)\n",
    "\n",
    "#<---- optinally check\n",
    "fb.check_env(testing_env) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perform testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing @ [ppo_model/baseZ]\n",
      "[.] Testing for [1] episodes @ [inf] steps\n",
      "\n",
      "[++] Begin Epoch: Running for 1 episodes\n",
      "\n",
      "[+] Begin Episode: 1 of 1\n",
      "[x] End Episode: 1] :: Return: -1.1040287017822266, Steps: 50\n",
      "[--] End Epoch [1] episodes :: Avg Return: -1.1040287017822266, Total Steps: 50.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTesting @ [\u001b[39m\u001b[39m{\u001b[39;00mmodel_path\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m average_return, total_steps, _ \u001b[39m=\u001b[39m fb\u001b[39m.\u001b[39mTEST(\n\u001b[1;32m      3\u001b[0m     env\u001b[39m=\u001b[39m            testing_env, \n\u001b[1;32m      4\u001b[0m     model\u001b[39m=\u001b[39m          sbalgo\u001b[39m.\u001b[39mload(model_path), \u001b[39m#<---- use None for random\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     episodes\u001b[39m=\u001b[39m       \u001b[39m1\u001b[39m,\n\u001b[1;32m      6\u001b[0m     steps\u001b[39m=\u001b[39m          \u001b[39m0\u001b[39m,\n\u001b[1;32m      7\u001b[0m     deterministic\u001b[39m=\u001b[39m  \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     render_as\u001b[39m=\u001b[39m      \u001b[39m'\u001b[39m\u001b[39mT\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m# use None for no plots, use '' (empty string) to plot inline\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     save_dpi\u001b[39m=\u001b[39m       \u001b[39m'\u001b[39m\u001b[39mfigure\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     make_video\u001b[39m=\u001b[39m     \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     video_fps\u001b[39m=\u001b[39m      \u001b[39m2\u001b[39m,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00maverage_return\u001b[39m=}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mtotal_steps\u001b[39m=}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "print(f'Testing @ [{model_path}]')\n",
    "average_return, total_steps, _ = fb.TEST(\n",
    "    env=            testing_env, \n",
    "    model=          sbalgo.load(model_path), #<---- use None for random\n",
    "    episodes=       1,\n",
    "    steps=          0,\n",
    "    deterministic=  True,\n",
    "    render_as=      'T', # use None for no plots, use '' (empty string) to plot inline\n",
    "    save_dpi=       'figure',\n",
    "    make_video=     False,\n",
    "    video_fps=      2,\n",
    ")\n",
    "print(f'{average_return=}, {total_steps=}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Welcome.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pie')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6951b015afe6d82d9b52a42a106328463243a98cf6ba49ab800da308edd1c19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
